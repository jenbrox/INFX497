{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenbrox/INFX497/blob/main/Housing3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db196381-59f2-402c-b50a-c3a421562c78",
      "metadata": {
        "id": "db196381-59f2-402c-b50a-c3a421562c78",
        "outputId": "cb7f8dd4-908a-4206-f29e-12d7e017dee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my path is  c:\\Users\\JB\\infx497\\ml\\ml_env\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"my path is \", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8330a77",
      "metadata": {
        "id": "f8330a77"
      },
      "source": [
        "# Chapter 2\n",
        "Your first task is to use California census data to build a model of housing prices in the state. This data includes metrics such as the population, median income, and median housing price for each\n",
        "dstrict in California. Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f8c507",
      "metadata": {
        "id": "f2f8c507"
      },
      "source": [
        "## Machine Learning Project Checklist\n",
        "\n",
        "1. Look at the big picture.\n",
        "2. Get the data.\n",
        "3. Discover and visualize the data to gain insights.\n",
        "4. Prepare the data for Machine Learning algorithms.\n",
        "5. Select a model and train it.\n",
        "6. Fine-tune your model.\n",
        "7. Present your solution.\n",
        "8. Launch, monitor, and maintain your system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fece5539",
      "metadata": {
        "id": "fece5539"
      },
      "source": [
        "### 1. Frame the problem and look at the big picture\n",
        "Model's output, a prediction of district's median housing price, will be ged to another ML system (pipeline) along with other information which will determine whether it is worth investing in the area or not.\n",
        "\n",
        "Ask yourself, is this a Supervised, Unsupervised, or Reinforcement Learning problem? Is it a calssification task, regression task, or something else? Should you use batch learning, online learning, or something else?\n",
        "\n",
        "1. Supervised\n",
        "- Regression: predicting a value\n",
        "- Classification: Predict a class\n",
        "2. Unsupervised\n",
        "- Clustering: Group ungrouped data\n",
        "- Assocation Rule Mining: Find associations between two or more classes\n",
        "3. Reinforcement\n",
        "- Supervised univariate regression task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9be6285-e147-453a-bef4-eb898cd7becf",
      "metadata": {
        "id": "c9be6285-e147-453a-bef4-eb898cd7becf"
      },
      "outputs": [],
      "source": [
        "#function that fetches housing.tgz data\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "#creates a datasets/housing directory in your workspace, downloads the housing.tgz file, and extracts the housing.csv file from it in this directory\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=\"HOUSING_PATH\"):\n",
        "    os.makedirs(housing_path, exist_ok=True)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66cfb93a-b1a1-4ac7-8da8-ce6718e764f7",
      "metadata": {
        "id": "66cfb93a-b1a1-4ac7-8da8-ce6718e764f7",
        "outputId": "7fa70171-650e-4856-be54-bb503d69eb9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JB\\AppData\\Local\\Temp\\ipykernel_8860\\1200238656.py:2: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "#load the data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "#returns a pandas DataFrame object containing all the data\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65846108-f255-423a-a22c-56f7b6b2f4d5",
      "metadata": {
        "id": "65846108-f255-423a-a22c-56f7b6b2f4d5",
        "outputId": "a7d96863-f226-4170-c4f7-174a0486a503"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasets\\\\housing\\\\housing.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#view top five rows in the dataset using head()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#10 attributes\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m housing \u001b[38;5;241m=\u001b[39m \u001b[43mload_housing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m housing\u001b[38;5;241m.\u001b[39mhead()\n",
            "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mload_housing_data\u001b[1;34m(housing_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_housing_data\u001b[39m(housing_path\u001b[38;5;241m=\u001b[39mHOUSING_PATH):\n\u001b[0;32m      6\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(housing_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousing.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets\\\\housing\\\\housing.csv'"
          ]
        }
      ],
      "source": [
        "#view top five rows in the dataset using head()\n",
        "#10 attributes\n",
        "housing = load_housing_data()\n",
        "housing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345bb50d-7dd9-4172-ab4b-c92b66155169",
      "metadata": {
        "id": "345bb50d-7dd9-4172-ab4b-c92b66155169"
      },
      "outputs": [],
      "source": [
        "#get a quick description of the data using info()\n",
        "housing.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6087f2ed-044c-4a47-aeef-6230b51963e2",
      "metadata": {
        "id": "6087f2ed-044c-4a47-aeef-6230b51963e2"
      },
      "outputs": [],
      "source": [
        "#only ocean_proximity is an object, but we know it is text since this is loaded from a CSV file\n",
        "#find out what categories exist and how many districts belong to each category using value_counts()\n",
        "print(housing[\"ocean_proximity\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303034e2-431f-48d5-bccd-410f73d26676",
      "metadata": {
        "id": "303034e2-431f-48d5-bccd-410f73d26676"
      },
      "outputs": [],
      "source": [
        "#summary of numerical attributes using describe() to get a feel for the data\n",
        "housing.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06886fe-6cf4-4976-8924-b95e2dbcb009",
      "metadata": {
        "scrolled": true,
        "id": "b06886fe-6cf4-4976-8924-b95e2dbcb009"
      },
      "outputs": [],
      "source": [
        "#plot a histogram to get a feel for the data\n",
        "#plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).\n",
        "#specify which backend Matplotlib, tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend.\n",
        "# only in a Jupyter notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50, figsize=(20,15))\n",
        "#optional in a Jupyter notebook, as Jupyter will automatically display plots when a cell is executed\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d81174d",
      "metadata": {
        "id": "7d81174d"
      },
      "source": [
        "# Create a test set\n",
        "Pick instances randomly, typically %20 of the testset. Don't look at data too much before setting aside some training data even before choosing an algorithm to avoid your own biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c8b7f6-39c2-49bc-a45b-3de77d0b5800",
      "metadata": {
        "id": "37c8b7f6-39c2-49bc-a45b-3de77d0b5800"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_train_test(data, test_ratio):\n",
        " shuffled_indices = np.random.permutation(len(data))\n",
        " test_set_size = int(len(data) * test_ratio)\n",
        " test_indices = shuffled_indices[:test_set_size]\n",
        " train_indices = shuffled_indices[test_set_size:]\n",
        " return data.iloc[train_indices], data.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ad8e5d-a4bf-4d53-a0cf-fa1397e37fd9",
      "metadata": {
        "id": "12ad8e5d-a4bf-4d53-a0cf-fa1397e37fd9"
      },
      "outputs": [],
      "source": [
        "#compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to\n",
        "#20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset.\n",
        "#The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.\n",
        "from zlib import crc32\n",
        "\n",
        "def test_set_check(identifier, test_ratio):\n",
        " return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
        "\n",
        "def split_train_test_by_id(data, test_ratio, id_column):\n",
        " ids = data[id_column]\n",
        " in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
        " return data.loc[~in_test_set], data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a685873a-1963-4319-8161-db5b80575996",
      "metadata": {
        "id": "a685873a-1963-4319-8161-db5b80575996"
      },
      "outputs": [],
      "source": [
        "#adds an `index` column\n",
        "# housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
        "# train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faeed314-f254-40ba-8450-504d46981932",
      "metadata": {
        "id": "faeed314-f254-40ba-8450-504d46981932"
      },
      "outputs": [],
      "source": [
        "#adds an 'index' column with sklearn with more features than above\n",
        "#random_state parameter that allows you to set the random generator seed\n",
        "#can pass it multiple datasets with an identical number of rows, and it will split them on the same indices\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e30db87-d631-462a-bf2e-c3cac13f762d",
      "metadata": {
        "id": "5e30db87-d631-462a-bf2e-c3cac13f762d"
      },
      "outputs": [],
      "source": [
        "#To reduce sampling bias, we implement strata samplying and ensure our data includes the median income to accurately predict median home prices\n",
        "#using the pd.cut() function to create an income category attribute with five categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5\n",
        "#(i.e., less than $15,000), category 2 from 1.5 to 3, etc\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\n",
        "housing[\"income_cat\"].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124a36d3-3639-4545-b85b-73710721e75d",
      "metadata": {
        "id": "124a36d3-3639-4545-b85b-73710721e75d"
      },
      "outputs": [],
      "source": [
        "#stratified sampling based on the income category using Scikit-Learn’s StratifiedShuffleSplit class:\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        " strat_train_set = housing.loc[train_index]\n",
        " strat_test_set = housing.loc[test_index]\n",
        "#look at income category proportions in the test set\n",
        "print(strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e958ba2-4599-4fbe-a01f-c01f6efb38a8",
      "metadata": {
        "id": "2e958ba2-4599-4fbe-a01f-c01f6efb38a8"
      },
      "outputs": [],
      "source": [
        "#remove the income_cat attribute so the data is back to its original state\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        " set_.drop(\"income_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b722d4-5c3c-4ee5-9686-62c21b166b25",
      "metadata": {
        "id": "e2b722d4-5c3c-4ee5-9686-62c21b166b25"
      },
      "source": [
        "# Discover and visualize the Data to Gain Insights\n",
        "## Page 56\n",
        "### This is only using the training set (put test data aside)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cc1dd8-0d9f-401e-b10b-b695cc3733d7",
      "metadata": {
        "id": "c5cc1dd8-0d9f-401e-b10b-b695cc3733d7"
      },
      "outputs": [],
      "source": [
        "#copy training set to play with\n",
        "housing = strat_train_set.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0067353a-2c34-4298-b081-c2e1eca834b6",
      "metadata": {
        "id": "0067353a-2c34-4298-b081-c2e1eca834b6"
      },
      "outputs": [],
      "source": [
        "#Visualize Geographical Data\n",
        "#housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
        "#set alpha option to visualize the places where there is a high density of data points\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8db7b6-019a-4d36-b583-2840f2b02625",
      "metadata": {
        "id": "0a8db7b6-019a-4d36-b583-2840f2b02625"
      },
      "outputs": [],
      "source": [
        "#now look at housing prices\n",
        "#each radius represents the district's population (s), color represents price (c)\n",
        "#use predefined  color map (cmap)\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2165e90d-c28f-4b98-b529-c4dda60e6f3c",
      "metadata": {
        "id": "2165e90d-c28f-4b98-b529-c4dda60e6f3c"
      },
      "outputs": [],
      "source": [
        "#compute standard correlation coefficient (Pearson's r) between every pair of attributes using corr()\n",
        "corr_matrix = housing.corr(numeric_only=True)\n",
        "#look at how each attribute correlates with median house value\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e65fcc-6d96-4ff8-b3a6-a78a5e7161aa",
      "metadata": {
        "id": "05e65fcc-6d96-4ff8-b3a6-a78a5e7161aa"
      },
      "source": [
        "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
        "there is a strong positive correlation; for example, the median house value tends to go\n",
        "up when the median income goes up. When the coefficient is close to –1, it means\n",
        "that there is a strong negative correlation; you can see a small negative correlation\n",
        "between the latitude and the median house value (i.e., prices have a slight tendency to\n",
        "go down when you go north). Finally, coefficients close to 0 mean that there is no\n",
        "linear correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8125879e-03b4-4771-b1dd-dd101ac77798",
      "metadata": {
        "id": "8125879e-03b4-4771-b1dd-dd101ac77798"
      },
      "outputs": [],
      "source": [
        "#another way to check for correlation between attributes is scatter_matrix()\n",
        "#which plots every numerical attribute against every other numerical attribute\n",
        "#limit to promising attributes\n",
        "from pandas.plotting import scatter_matrix\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce2895a-84ad-4191-9043-c4371d2e2d1f",
      "metadata": {
        "id": "fce2895a-84ad-4191-9043-c4371d2e2d1f"
      },
      "outputs": [],
      "source": [
        "#zoom in on the most promising attribute, median income\n",
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4eb029-06bf-47e2-a26d-147d97a67e26",
      "metadata": {
        "id": "3d4eb029-06bf-47e2-a26d-147d97a67e26"
      },
      "outputs": [],
      "source": [
        "#combine some attributes\n",
        "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
        "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
        "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
        "#look at the correlation matrix again:\n",
        "corr_matrix = housing.corr(numeric_only=True)\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189ce435-2580-4eba-9af3-521fc01d7cb9",
      "metadata": {
        "id": "189ce435-2580-4eba-9af3-521fc01d7cb9"
      },
      "source": [
        "# Prepare the Data for Machine Learning Algorithms\n",
        "### Page 62"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d395377-2c77-4384-b464-f5603afa9ef6",
      "metadata": {
        "id": "6d395377-2c77-4384-b464-f5603afa9ef6"
      },
      "outputs": [],
      "source": [
        "#revert to a clean training set\n",
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ef6b1c-4142-4ef2-9d93-a9647b164b39",
      "metadata": {
        "id": "63ef6b1c-4142-4ef2-9d93-a9647b164b39"
      },
      "outputs": [],
      "source": [
        "#Data Cleaning\n",
        "#Fix missing features since Machine Learning Algorithms cannot work with that\n",
        "#use SimpleImputer to take care of missing values by replacing each missing value with the median of that attribute\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "#Since the median can only be computed on numerical attributes create a copy of the data without the text attribute ocean_proximity\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
        "imputer.fit(housing_num)\n",
        "print(imputer.statistics_)\n",
        "print(housing_num.median().values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af88cec-4d5c-4d31-9453-dff705d4d5c7",
      "metadata": {
        "id": "7af88cec-4d5c-4d31-9453-dff705d4d5c7"
      },
      "outputs": [],
      "source": [
        "#now use the trained imputer to transform the training set by replacing missing values with learned medians\n",
        "X = imputer.transform(housing_num)\n",
        "#transofrm play NumPy array to DataFrame\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab73cc2-0b5f-49f0-baab-4e6d96462e60",
      "metadata": {
        "id": "bab73cc2-0b5f-49f0-baab-4e6d96462e60"
      },
      "outputs": [],
      "source": [
        "#Handling Text Attributes\n",
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29f2a85",
      "metadata": {
        "id": "a29f2a85"
      },
      "outputs": [],
      "source": [
        "#Convert text to numbers for Machine Learning Algorithms using OrdinalEncoder class\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5266b7f8-6045-4520-b9a4-8c334e8e1952",
      "metadata": {
        "id": "5266b7f8-6045-4520-b9a4-8c334e8e1952"
      },
      "outputs": [],
      "source": [
        "#get a list of categories using categories_ instance variable\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder.categories_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f352134e-5ca4-4926-9280-f6ccddf6394c",
      "metadata": {
        "id": "f352134e-5ca4-4926-9280-f6ccddf6394c"
      },
      "outputs": [],
      "source": [
        "# ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g.,\n",
        "#for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obviously not the case for the ocean_proximity column\n",
        "#(for example, categories 0 and 4are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary\n",
        "#attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the cate‐\n",
        "#gory is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the\n",
        "#others will be 0 (cold). The new attributes are sometimes called dummy attributes. Uses oneHotEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot\n",
        "#or to convert to a dense NumPy array\n",
        "#housing_cat_1hot.toarray()\n",
        "cat_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7979167f-6923-4624-a364-d7c0a243400c",
      "metadata": {
        "id": "7979167f-6923-4624-a364-d7c0a243400c"
      },
      "source": [
        "# Custom Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8144e869-d945-4e78-a43e-a3a141ddeed3",
      "metadata": {
        "id": "8144e869-d945-4e78-a43e-a3a141ddeed3"
      },
      "outputs": [],
      "source": [
        "#we will need to write our own for tasks such as custom cleanup or combining specific attributes.\n",
        "#create a class and implement three methods, fit(), transform(), fit_transform()\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        " def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
        "     self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        " def fit(self, X, y=None):\n",
        "     return self # nothing else to do\n",
        " def transform(self, X):\n",
        "     rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "     population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "     if self.add_bedrooms_per_room:\n",
        "         bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "         return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
        "     else:\n",
        "        return np.c_[X, rooms_per_household, population_per_household]\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c7d564-4534-4015-8430-05d6b8a0c0f5",
      "metadata": {
        "id": "64c7d564-4534-4015-8430-05d6b8a0c0f5"
      },
      "source": [
        "## Feature Scaling\n",
        "Not generally required. There are two common ways to get all attributes to have the same scale: min-max\n",
        "scaling and standardizatio\n",
        "## TransformationPipelinesn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e27f91-3fd1-465e-bc9b-d3a0c0b22a5d",
      "metadata": {
        "id": "96e27f91-3fd1-465e-bc9b-d3a0c0b22a5d"
      },
      "outputs": [],
      "source": [
        "#many data transformation steps need to be executed in the right order, use Pipeline class to help with sequences\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "num_pipeline = Pipeline([\n",
        " ('imputer', SimpleImputer(strategy=\"median\")),\n",
        " ('attribs_adder', CombinedAttributesAdder()),\n",
        " ('std_scaler', StandardScaler()),\n",
        " ])\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57bd0785-f846-4291-84a0-2d22c5b877ad",
      "metadata": {
        "id": "57bd0785-f846-4291-84a0-2d22c5b877ad"
      },
      "outputs": [],
      "source": [
        "#apply all the transformations to the housing data\n",
        "from sklearn.compose import ColumnTransformer\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "full_pipeline = ColumnTransformer([\n",
        " (\"num\", num_pipeline, num_attribs),\n",
        " (\"cat\", OneHotEncoder(), cat_attribs),\n",
        " ])\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee95d55-1add-4f91-bd93-51302e1c5aad",
      "metadata": {
        "id": "7ee95d55-1add-4f91-bd93-51302e1c5aad"
      },
      "source": [
        "# Select and Train a Model\n",
        "## Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25444b9-2739-48e8-832a-d28e5073fa27",
      "metadata": {
        "id": "d25444b9-2739-48e8-832a-d28e5073fa27"
      },
      "outputs": [],
      "source": [
        "#first train a Linear Regression model\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b0bcf1-0d0b-4b7d-be8d-3520231db480",
      "metadata": {
        "id": "13b0bcf1-0d0b-4b7d-be8d-3520231db480"
      },
      "outputs": [],
      "source": [
        "#try the Linear Regression model out on a few instances from training set\n",
        "some_data = housing.iloc[:5]\n",
        "some_labels = housing_labels.iloc[:5]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
        "print(\"Labels:\", list(some_labels))\n",
        "# predictions are not exactly accurate (e.g., the first prediction is off by close to 40%!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db82534-af1a-4c1b-90d2-37bd8788403f",
      "metadata": {
        "id": "8db82534-af1a-4c1b-90d2-37bd8788403f"
      },
      "outputs": [],
      "source": [
        "#Measure this regression model’s RMSE on the whole training set using mean_squared_error() function\n",
        "from sklearn.metrics import mean_squared_error\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse\n",
        "#most districts’ median_housing_values range between $120,000 and $265,000, so a typical prediction error of\n",
        "#$68,628 is not very satisfying. This is an example of a model underfitting the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de72e000-0b29-4465-95dc-8061373988d4",
      "metadata": {
        "id": "de72e000-0b29-4465-95dc-8061373988d4"
      },
      "outputs": [],
      "source": [
        "#train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(housing_prepared, housing_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a961c92-51b8-4c1d-9e1a-bd0ed0414703",
      "metadata": {
        "id": "1a961c92-51b8-4c1d-9e1a-bd0ed0414703"
      },
      "outputs": [],
      "source": [
        "#Now that the model is trained, let’s evaluate it on the training set:\n",
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse\n",
        "#model too perfect (0.0), may have overfit the model now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad17515-2770-4de9-982d-96d008adeebb",
      "metadata": {
        "id": "6ad17515-2770-4de9-982d-96d008adeebb"
      },
      "outputs": [],
      "source": [
        "#use K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it\n",
        "#trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds.\n",
        "#The result is an array containing the 10 evaluation scores:\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        " scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "#print\n",
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "display_scores(tree_rmse_scores)\n",
        "#the Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b805d29-8ff4-4e1f-928d-2c667d6d4a50",
      "metadata": {
        "id": "8b805d29-8ff4-4e1f-928d-2c667d6d4a50"
      },
      "outputs": [],
      "source": [
        "#try RandomForestRegressor model\n",
        "#works by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many\n",
        "#other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
        "forest_reg.fit(housing_prepared, housing_labels)\n",
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "#print(display_scores(forest_rmse_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "191a0eff-2612-4683-87ea-10735abe4894",
      "metadata": {
        "id": "191a0eff-2612-4683-87ea-10735abe4894"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "print(display_scores(forest_rmse_scores))\n",
        "#much better than the rest but still overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d1d3146-6048-4dab-a7aa-9528f01bbe46",
      "metadata": {
        "id": "2d1d3146-6048-4dab-a7aa-9528f01bbe46"
      },
      "source": [
        "# Chapter 2, Exercise 1\n",
        "Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\"linear\" (with various values for the C\n",
        "hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now.\n",
        "How does the best SVR predictor perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b198c915-f1df-4556-ba49-5b10a2fce9c8",
      "metadata": {
        "id": "b198c915-f1df-4556-ba49-5b10a2fce9c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05947b62-ebe7-429a-9fc4-4737a4656748",
      "metadata": {
        "id": "05947b62-ebe7-429a-9fc4-4737a4656748"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb9bceb-490c-48f9-bc5a-a89da1036ba4",
      "metadata": {
        "id": "6bb9bceb-490c-48f9-bc5a-a89da1036ba4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07bc6f57-9a47-4281-9ad9-6592fe510eee",
      "metadata": {
        "id": "07bc6f57-9a47-4281-9ad9-6592fe510eee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1282a701-c87a-415b-8dac-ee4380f670a4",
      "metadata": {
        "id": "1282a701-c87a-415b-8dac-ee4380f670a4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e125fc-ec51-41a9-97cd-c9468ea2bb5c",
      "metadata": {
        "id": "a2e125fc-ec51-41a9-97cd-c9468ea2bb5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b07dffa6-742b-4edd-8b09-c3d05b7ddf57",
      "metadata": {
        "id": "b07dffa6-742b-4edd-8b09-c3d05b7ddf57"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7165bda7-c81c-4c1a-b42b-6553ed69683f",
      "metadata": {
        "id": "7165bda7-c81c-4c1a-b42b-6553ed69683f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aadfd7f-6b59-434d-92d5-3b6d1cb6c1d0",
      "metadata": {
        "id": "0aadfd7f-6b59-434d-92d5-3b6d1cb6c1d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f0cc63e2-0da2-49c7-8a26-5abfe2543fa4",
      "metadata": {
        "id": "f0cc63e2-0da2-49c7-8a26-5abfe2543fa4"
      },
      "source": [
        "# Chapter 2, Exercise 2\n",
        "Try replacing GridSearchCV with RandomizedSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55773b98-64c2-47b7-9fa3-fdbd497ce16a",
      "metadata": {
        "id": "55773b98-64c2-47b7-9fa3-fdbd497ce16a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f4bf99-6998-46c3-86d9-efdc7e130596",
      "metadata": {
        "id": "65f4bf99-6998-46c3-86d9-efdc7e130596"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcea1202-5d02-42e7-8b75-226ff2a11df4",
      "metadata": {
        "id": "bcea1202-5d02-42e7-8b75-226ff2a11df4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7708c511-c2f9-43ef-96e3-374705d3821c",
      "metadata": {
        "id": "7708c511-c2f9-43ef-96e3-374705d3821c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b9997d-3571-4930-bc57-e470dd76ee01",
      "metadata": {
        "id": "05b9997d-3571-4930-bc57-e470dd76ee01"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08cdaed-987e-43d5-bcfa-a0d22a9952ae",
      "metadata": {
        "id": "a08cdaed-987e-43d5-bcfa-a0d22a9952ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6e2632-722b-4c76-9b65-ae2f7c3103de",
      "metadata": {
        "id": "7e6e2632-722b-4c76-9b65-ae2f7c3103de"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ebfd16f4-3252-4baa-bd94-91411bccb87c",
      "metadata": {
        "id": "ebfd16f4-3252-4baa-bd94-91411bccb87c"
      },
      "source": [
        "# Chapter 2, Exercise 3\n",
        "Try adding a transformer in the preparation pipeline to select only the most important attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574569c7-fbba-41fb-aba8-fed0e38b83f4",
      "metadata": {
        "id": "574569c7-fbba-41fb-aba8-fed0e38b83f4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44008bd",
      "metadata": {
        "id": "e44008bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889b04ea",
      "metadata": {
        "id": "889b04ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25fb8ea",
      "metadata": {
        "id": "f25fb8ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209336bc",
      "metadata": {
        "id": "209336bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4889d36d",
      "metadata": {
        "id": "4889d36d"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba520a7a",
      "metadata": {
        "id": "ba520a7a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ded24d",
      "metadata": {
        "id": "b1ded24d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e993ef",
      "metadata": {
        "id": "31e993ef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "564c301f-30c3-4d0d-8174-8344950eefb9",
      "metadata": {
        "id": "564c301f-30c3-4d0d-8174-8344950eefb9"
      },
      "source": [
        "# Chapter 2, Exercise 4\n",
        "Try creating a single pipeline that does the full data preparation plus the final prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf60ed9-a2f2-4cee-93a7-4553e57d64c6",
      "metadata": {
        "id": "eaf60ed9-a2f2-4cee-93a7-4553e57d64c6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4affec1d",
      "metadata": {
        "id": "4affec1d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b73761",
      "metadata": {
        "id": "36b73761"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "183d1127-817c-4b3b-99e1-42581324a5d7",
      "metadata": {
        "id": "183d1127-817c-4b3b-99e1-42581324a5d7"
      },
      "source": [
        "# Chapter 2, Exercise 5\n",
        "Automatically explore some preparation options using GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5701a602-dc82-4389-852e-9dd811a72bcb",
      "metadata": {
        "id": "5701a602-dc82-4389-852e-9dd811a72bcb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e192134-abe1-4b16-822b-a1c5d58d0dcf",
      "metadata": {
        "id": "4e192134-abe1-4b16-822b-a1c5d58d0dcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "efe0cc7c-7858-4160-a09e-5b0ec48eaa16",
      "metadata": {
        "id": "efe0cc7c-7858-4160-a09e-5b0ec48eaa16"
      },
      "source": [
        "# Chapter 3\n",
        "## Using MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b61cb0-ae77-4b8b-a431-a4c0237bd751",
      "metadata": {
        "id": "78b61cb0-ae77-4b8b-a431-a4c0237bd751"
      },
      "outputs": [],
      "source": [
        "# Import the function to fetch datasets from OpenML, an online repository of well-documented datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Fetch MNIST dataset containing handwritten digits with 784 features\n",
        "mnist = fetch_openml('mnist_784', version=1,as_frame=False)\n",
        "\n",
        "# keys of the MNIST dataset. object contains the data, target (labels), a description of the dataset, and other metadata.\n",
        "# keys: 'data': the feature matrix,'target': the label array,'feature_names': the names of the features,'DESCR': a full description of the dataset,\n",
        "# 'categories': the category labels (for categorical features)\n",
        "mnist_keys = mnist.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ea433d-3c1d-4897-ab04-3a394362cef8",
      "metadata": {
        "id": "88ea433d-3c1d-4897-ab04-3a394362cef8"
      },
      "outputs": [],
      "source": [
        "# examine the structure of the data (X) and labels (y)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "#return the dimensions of the array containing the image data\n",
        "X.shape\n",
        "#return the dimensions of the array containing the labels for the images\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc25f095-a81f-4163-9879-2147d6cb32e7",
      "metadata": {
        "id": "fc25f095-a81f-4163-9879-2147d6cb32e7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#an instance’s feature vector, reshape it to a 28 × 28 array, and display it using Matplotlib’s imshow() function:\n",
        "some_digit = X[0]\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "plt.imshow(some_digit_image, cmap=\"binary\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc165995-1c55-47e4-8e19-09b8209335a2",
      "metadata": {
        "id": "fc165995-1c55-47e4-8e19-09b8209335a2"
      },
      "outputs": [],
      "source": [
        "#double check the label to make sure it is what we think it is\n",
        "y[0]\n",
        "#cast the string label to an int since that is what most ML algorithms expect\n",
        "y = y.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faddc09d-f6a0-4cf7-b4a7-968adc27ae84",
      "metadata": {
        "id": "faddc09d-f6a0-4cf7-b4a7-968adc27ae84"
      },
      "outputs": [],
      "source": [
        "#create a test set first (for this datsset using already given split)\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e6c677-8043-4a2e-801d-0b39b29c1e92",
      "metadata": {
        "id": "d5e6c677-8043-4a2e-801d-0b39b29c1e92"
      },
      "source": [
        "## Training a Binary Classifier\n",
        "Distinguishes between two classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d62a537f-a5f5-43ce-ad1c-6664a4d93a61",
      "metadata": {
        "id": "d62a537f-a5f5-43ce-ad1c-6664a4d93a61"
      },
      "outputs": [],
      "source": [
        "#target vectors for this classification task\n",
        "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits\n",
        "y_test_5 = (y_test == 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4feca3b-7875-46e1-9456-82cc99500d97",
      "metadata": {
        "id": "b4feca3b-7875-46e1-9456-82cc99500d97"
      },
      "outputs": [],
      "source": [
        "# pick a classifier and train it\n",
        "# stochastic gradient descent classifier, train it to differentiate between the digit '5' and other digits in the MNIST dataset\n",
        "#The X_train contains the training data features, while y_train_5 is the target variable for the binary classification task\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "#random_state gives you reproducible results\n",
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train_5)\n",
        "#detect images of the number 5\n",
        "sgd_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53be1e58-5a1c-40fb-b293-009c8702a3e3",
      "metadata": {
        "id": "53be1e58-5a1c-40fb-b293-009c8702a3e3"
      },
      "source": [
        "## Performance Measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49108cc-59df-49da-ac2f-cc749c72a6e0",
      "metadata": {
        "id": "d49108cc-59df-49da-ac2f-cc749c72a6e0"
      },
      "outputs": [],
      "source": [
        "#meausry accuracy Using Cross-Validation\n",
        "#evaluates the model\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import clone\n",
        "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
        " clone_clf = clone(sgd_clf)\n",
        " X_train_folds = X_train[train_index]\n",
        " y_train_folds = y_train_5[train_index]\n",
        " X_test_fold = X_train[test_index]\n",
        " y_test_fold = y_train_5[test_index]\n",
        " clone_clf.fit(X_train_folds, y_train_folds)\n",
        " y_pred = clone_clf.predict(X_test_fold)\n",
        " n_correct = sum(y_pred == y_test_fold)\n",
        " print(n_correct / len(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97624b2d-7eca-4235-b01a-14ce275752b4",
      "metadata": {
        "id": "97624b2d-7eca-4235-b01a-14ce275752b4"
      },
      "outputs": [],
      "source": [
        "#Use cross_val_score() to evaluate SGDClassifier. using K-fold cross-validation with three folds, ie splitting the training set into K folds\n",
        "#then making predictions and evaluating them on each fold using a model trained on the remaining folds\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
        "# Above 93% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838a6080-9d27-415c-91ce-162e3546ca9e",
      "metadata": {
        "id": "838a6080-9d27-415c-91ce-162e3546ca9e"
      },
      "outputs": [],
      "source": [
        "#very dumb classifier that just classifies every single image in the “not-5” class:\n",
        "from sklearn.base import BaseEstimator\n",
        "class Never5Classifier(BaseEstimator):\n",
        " def fit(self, X, y=None):\n",
        "     return self\n",
        " def predict(self, X):\n",
        "     return np.zeros((len(X), 1), dtype=bool)\n",
        "\n",
        "never_5_clf = Never5Classifier()\n",
        "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
        "#90% accuracy bc about 10% of the images are 5, so not too hard to do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c855880-4bd9-45ef-8052-e53bdb8aaac5",
      "metadata": {
        "id": "9c855880-4bd9-45ef-8052-e53bdb8aaac5"
      },
      "outputs": [],
      "source": [
        "#Great example as to why accuracy is not the preferred performance measure for classifiers\n",
        "#A better way is a Confusion Matrix\n",
        "#like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores,\n",
        "#it returns the predictions made on each test fold.\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
        "# get the confusion matrix using the confusion_matrix() function. pass it the target classes (y_train_5) and the predicted classes (y_train_pred):\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_train_5, y_train_pred)\n",
        "#Each row in a confusion matrix represents an actual class, while each column represents a predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6609fc5f-b5bd-4f83-9674-f25197f05723",
      "metadata": {
        "id": "6609fc5f-b5bd-4f83-9674-f25197f05723"
      },
      "outputs": [],
      "source": [
        "#comute classifier metrics, measurements used to evaluate the performance of a classification model\n",
        "#compute the harmonic mean of precision and recall/(F_1)\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_train_5, y_train_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31cebc34-88c3-49b7-9086-b81809c17409",
      "metadata": {
        "id": "31cebc34-88c3-49b7-9086-b81809c17409"
      },
      "outputs": [],
      "source": [
        "#this decision_function method returns the decision score for each instance\n",
        "y_scores = sgd_clf.decision_function([some_digit])\n",
        "y_scores\n",
        "threshold = 0\n",
        "y_some_digit_pred = (y_scores > threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91f83279-4349-4d14-9a55-ddaf5b680d37",
      "metadata": {
        "id": "91f83279-4349-4d14-9a55-ddaf5b680d37"
      },
      "outputs": [],
      "source": [
        "#The SGDClassifier uses a threshold equal to 0, so the previous code returns the same result as the predict() method (i.e., True).\n",
        "#raise the threshold:\n",
        "threshold = 8000\n",
        "y_some_digit_pred = (y_scores > threshold)\n",
        "y_some_digit_pred\n",
        "#raising threshold reduces recall and classifier no longer detects it as a 5 as it did when threshold = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bd479c-e513-412d-9a37-9f62a035bf31",
      "metadata": {
        "id": "e6bd479c-e513-412d-9a37-9f62a035bf31"
      },
      "outputs": [],
      "source": [
        "#decdide what threshold to use:\n",
        "#use cross_val_predict() function to get the scores of all instances in the training set,\n",
        "#specify that you want to return decision scores instead of predictions:\n",
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7098509-93e7-48ba-b036-614f4a3e6848",
      "metadata": {
        "id": "d7098509-93e7-48ba-b036-614f4a3e6848"
      },
      "outputs": [],
      "source": [
        "#Then with these scores, use the precision_recall_curve() function to compute precision and recall for all possible thresholds:\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae077e9-e83f-4398-b267-ba5211387005",
      "metadata": {
        "id": "1ae077e9-e83f-4398-b267-ba5211387005"
      },
      "outputs": [],
      "source": [
        "# Matplotlib to plot precision and recall as functions of the threshold value\n",
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "    plt.legend(loc=\"center right\", fontsize=16)\n",
        "    plt.xlabel(\"Threshold\", fontsize=16)\n",
        "    plt.grid(True)\n",
        "    plt.axis([-50000, 50000, 0, 1])\n",
        "\n",
        "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
        "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")\n",
        "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")\n",
        "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")\n",
        "plt.plot([threshold_90_precision], [0.9], \"ro\")\n",
        "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#     [...] # highlight the threshold and add the legend, axis label, and grid\n",
        "#     # Highlight the threshold (e.g., with a red vertical line at the chosen threshold value)\n",
        "#     chosen_threshold = 5000\n",
        "#     plt.plot([chosen_threshold, chosen_threshold], [0, 1], \"r:\")     # Threshold line\n",
        "#     plt.annotate('Threshold', xy=(chosen_threshold, 0.5), xytext=(chosen_threshold+500, 0.5), arrowprops=dict(facecolor='black', shrink=0.05), )\n",
        "#     plt.xlabel(\"Threshold\")                                           # x-axis label\n",
        "#     plt.ylabel(\"Score\")                                               # y-axis label\n",
        "#     plt.legend()                                                      # Show legend\n",
        "#     plt.grid(True)                                                    # Turn on the grid\n",
        "#     plt.xlim([-50000, 50000])\n",
        "\n",
        "# plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72a2f68-eb14-4962-bb4d-d9e964396d23",
      "metadata": {
        "id": "c72a2f68-eb14-4962-bb4d-d9e964396d23"
      },
      "outputs": [],
      "source": [
        "(y_train_pred == (y_scores > 0)).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad31b99-c037-439d-81af-dd633d18f774",
      "metadata": {
        "id": "1ad31b99-c037-439d-81af-dd633d18f774"
      },
      "outputs": [],
      "source": [
        "def plot_precision_vs_recall(precisions, recalls):\n",
        "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
        "    plt.xlabel(\"Recall\", fontsize=16)\n",
        "    plt.ylabel(\"Precision\", fontsize=16)\n",
        "    plt.axis([0, 1, 0, 1])\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_precision_vs_recall(precisions, recalls)\n",
        "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\n",
        "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
        "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4bc61fb-fc54-4c15-86ea-d7d73cd45fb1",
      "metadata": {
        "id": "a4bc61fb-fc54-4c15-86ea-d7d73cd45fb1"
      },
      "outputs": [],
      "source": [
        "#if we want a 90% precision\n",
        "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c3d9dc-cb8d-48a8-970d-7beac621dc2f",
      "metadata": {
        "id": "a3c3d9dc-cb8d-48a8-970d-7beac621dc2f"
      },
      "outputs": [],
      "source": [
        "threshold_90_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9605e1-d7b6-4f7d-ac40-b86ccde89c54",
      "metadata": {
        "id": "ca9605e1-d7b6-4f7d-ac40-b86ccde89c54"
      },
      "outputs": [],
      "source": [
        "#to make predicitions instead of calling classifier's predicti() method\n",
        "y_train_pred_90 = (y_scores >= threshold_90_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79cf2ba5-abe0-4b3b-83ab-1db8f5aa45ad",
      "metadata": {
        "id": "79cf2ba5-abe0-4b3b-83ab-1db8f5aa45ad"
      },
      "outputs": [],
      "source": [
        "#check precision\n",
        "precision_score(y_train_5, y_train_pred_90)\n",
        "#this confirms 90% precision we wanted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5927c5-52f7-4151-a24b-ba227fc29b16",
      "metadata": {
        "id": "8e5927c5-52f7-4151-a24b-ba227fc29b16"
      },
      "outputs": [],
      "source": [
        "#check recall\n",
        "recall_score(y_train_5, y_train_pred_90)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b998d8-43c5-4334-a6b6-72035951c7c4",
      "metadata": {
        "id": "94b998d8-43c5-4334-a6b6-72035951c7c4"
      },
      "source": [
        "## The ROC Curve\n",
        "Receiver Operating Characteristic. Another classifier metric used with binary classifiers. Plots TPR (recall) against FPR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8521ad5b-e7e9-43a3-9bfc-9e65d5a75c1c",
      "metadata": {
        "id": "8521ad5b-e7e9-43a3-9bfc-9e65d5a75c1c"
      },
      "outputs": [],
      "source": [
        "#Compute TPR and FPR\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60bc07ce-5c44-449e-bf16-46ab7343c216",
      "metadata": {
        "id": "60bc07ce-5c44-449e-bf16-46ab7343c216"
      },
      "outputs": [],
      "source": [
        "#then plot TPR and FPR\n",
        "def plot_roc_curve(fpr, tpr, label=None):\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
        "    plt.axis([0, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_roc_curve(fpr, tpr)\n",
        "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]\n",
        "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")\n",
        "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\n",
        "plt.plot([fpr_90], [recall_90_precision], \"ro\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983d2045-2fc1-4d5e-a71e-5559dafffd0f",
      "metadata": {
        "id": "983d2045-2fc1-4d5e-a71e-5559dafffd0f"
      },
      "outputs": [],
      "source": [
        "#The more your ROC line is above the randoming guessing line the better. Use AUC to calculate this. 0.5 is purely random, 1 is perfect\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_score(y_train_5, y_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf44bedf-fa8b-48b0-a556-f0d1687f6901",
      "metadata": {
        "id": "cf44bedf-fa8b-48b0-a556-f0d1687f6901"
      },
      "outputs": [],
      "source": [
        "#Train RandomForestClassifer and compare it's ROC curve and ROC AUC score to the SGD Classifier\n",
        "#The predict_proba() method returns an array containing a row per instance and a column per class, each containing the probability that the given instance belongs to the given class (e.g., 70% chance that the image represents a 5):\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
        "                                    method=\"predict_proba\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aca476a-ec4a-443b-b049-a549e8d1aed5",
      "metadata": {
        "id": "3aca476a-ec4a-443b-b049-a549e8d1aed5"
      },
      "outputs": [],
      "source": [
        "#roc_curve() function expects labels and scores, but instead of scores you can give it class probabilities. Let’s use the positive class’s probability as the score:\n",
        "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
        "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd57d16-0821-48e1-be3e-3158b8411a21",
      "metadata": {
        "id": "2fd57d16-0821-48e1-be3e-3158b8411a21"
      },
      "outputs": [],
      "source": [
        "#plot ROC curve  to see how they compare\n",
        "recall_for_forest = tpr_forest[np.argmax(fpr_forest >= fpr_90)]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
        "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
        "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")\n",
        "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\n",
        "plt.plot([fpr_90], [recall_90_precision], \"ro\")\n",
        "plt.plot([fpr_90, fpr_90], [0., recall_for_forest], \"r:\")\n",
        "plt.plot([fpr_90], [recall_for_forest], \"ro\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa18220-df76-4ee8-876e-6fcedd8f45d8",
      "metadata": {
        "id": "0aa18220-df76-4ee8-876e-6fcedd8f45d8"
      },
      "outputs": [],
      "source": [
        "#RandomForest is better bc ROC Curve looks much better than SGDClassifier's as it comes much closer to top-left corner. As a result its ROC AUC score is also signifigantly better\n",
        "roc_auc_score(y_train_5, y_scores_forest)\n",
        "#99% precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc1fcb0-8d67-4822-89db-26e301b8cbae",
      "metadata": {
        "id": "dfc1fcb0-8d67-4822-89db-26e301b8cbae"
      },
      "outputs": [],
      "source": [
        "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
        "precision_score(y_train_5, y_train_pred_forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62b3af9-1e44-4fd7-b12c-f5cc0df7c194",
      "metadata": {
        "id": "f62b3af9-1e44-4fd7-b12c-f5cc0df7c194"
      },
      "outputs": [],
      "source": [
        "recall_score(y_train_5, y_train_pred_forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033fa048-50cb-4320-a646-c15dede5643a",
      "metadata": {
        "id": "033fa048-50cb-4320-a646-c15dede5643a"
      },
      "outputs": [],
      "source": [
        "## Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c84592b-33e7-4d87-afb2-998617c13a31",
      "metadata": {
        "id": "3c84592b-33e7-4d87-afb2-998617c13a31"
      },
      "outputs": [],
      "source": [
        "#Run a Support Vector Machine CLassifier (this algorithm is strictly binary)\n",
        "#trains the SVC on the training set using the 0-9 target classes, not 5 versus the rest (y_train_5)\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
        "svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5\n",
        "svm_clf.predict([some_digit])\n",
        "#this actually trained 45 binary classifiers,got their deicision scores for the image, and selected teh class that won the most duels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402322b9-6adc-48f2-aee0-1fa63f11ec80",
      "metadata": {
        "id": "402322b9-6adc-48f2-aee0-1fa63f11ec80"
      },
      "outputs": [],
      "source": [
        "#shown by returning the scores per instance\n",
        "some_digit_scores = svm_clf.decision_function([some_digit])\n",
        "some_digit_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3d47a3-628e-4b49-8a82-cf4a062c0563",
      "metadata": {
        "id": "9a3d47a3-628e-4b49-8a82-cf4a062c0563"
      },
      "outputs": [],
      "source": [
        "#highest score is one correspeonding to class 5\n",
        "np.argmax(some_digit_scores)\n",
        "svm_clf.classes_\n",
        "svm_clf.classes_[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea24c2d-aa35-48a8-bbae-138c961ed104",
      "metadata": {
        "id": "fea24c2d-aa35-48a8-bbae-138c961ed104"
      },
      "outputs": [],
      "source": [
        "#to force skikit to use a certain classifier (OvO, OvR, etc\n",
        "#this creates a multiclass classifier using the OvR strategy based on SVR\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
        "ovr_clf.fit(X_train[:1000], y_train[:1000])\n",
        "ovr_clf.predict([some_digit])\n",
        "len(ovr_clf.estimators_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5318d590-2be3-4531-b312-7b08a12007a7",
      "metadata": {
        "id": "5318d590-2be3-4531-b312-7b08a12007a7"
      },
      "outputs": [],
      "source": [
        "#Or using the SGDClassifier\n",
        "sgd_clf.fit(X_train, y_train)\n",
        "sgd_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcac95fb-5004-4136-93d2-836e64b24c8f",
      "metadata": {
        "id": "fcac95fb-5004-4136-93d2-836e64b24c8f"
      },
      "outputs": [],
      "source": [
        "#since Skikit used the OvR strategy, there are now 10 classes and it trained 10 binary classifiers. THis now returns one value per class\n",
        "#score that SGD Classifier assigned to each class\n",
        "sgd_clf.decision_function([some_digit])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672f1dce-8518-482e-a9e1-0e20666e1ac9",
      "metadata": {
        "id": "672f1dce-8518-482e-a9e1-0e20666e1ac9"
      },
      "outputs": [],
      "source": [
        "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c0fcf2-d2c0-47ee-87d2-60d6cf935884",
      "metadata": {
        "id": "e5c0fcf2-d2c0-47ee-87d2-60d6cf935884"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
        "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca79cb4f-dbf3-4030-81e2-738348a08882",
      "metadata": {
        "id": "ca79cb4f-dbf3-4030-81e2-738348a08882"
      },
      "source": [
        "## Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65139672-a715-4ca8-a121-6f8e216da315",
      "metadata": {
        "id": "65139672-a715-4ca8-a121-6f8e216da315"
      },
      "outputs": [],
      "source": [
        "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
        "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
        "conf_mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e10c16-4837-455c-ae87-f3fdc6e7a255",
      "metadata": {
        "id": "85e10c16-4837-455c-ae87-f3fdc6e7a255"
      },
      "outputs": [],
      "source": [
        "# since sklearn 0.22, you can use sklearn.metrics.plot_confusion_matrix()\n",
        "def plot_confusion_matrix(matrix):\n",
        "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(matrix)\n",
        "    fig.colorbar(cax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15bc1fe7-fcb7-42c5-955f-cb3bc6b9c363",
      "metadata": {
        "id": "15bc1fe7-fcb7-42c5-955f-cb3bc6b9c363"
      },
      "outputs": [],
      "source": [
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "save_fig(\"confusion_matrix_plot\", tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fead3d-8f17-4f87-811e-6be225081b47",
      "metadata": {
        "id": "14fead3d-8f17-4f87-811e-6be225081b47"
      },
      "outputs": [],
      "source": [
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec405b1d-dfff-4a97-80d6-14c0ac462907",
      "metadata": {
        "id": "ec405b1d-dfff-4a97-80d6-14c0ac462907"
      },
      "outputs": [],
      "source": [
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd930ae-3ab7-437b-b33d-9b0a34ab3632",
      "metadata": {
        "id": "0dd930ae-3ab7-437b-b33d-9b0a34ab3632"
      },
      "outputs": [],
      "source": [
        "cl_a, cl_b = 3, 5\n",
        "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
        "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
        "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
        "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
        "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
        "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
        "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
        "save_fig(\"error_analysis_digits_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9067ed3-2b8f-442a-96ca-bd15071e8842",
      "metadata": {
        "id": "f9067ed3-2b8f-442a-96ca-bd15071e8842"
      },
      "source": [
        "## Multilabel Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81ba21e-1825-4a2b-bea1-87383f62f858",
      "metadata": {
        "id": "e81ba21e-1825-4a2b-bea1-87383f62f858"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "y_train_large = (y_train >= 7)\n",
        "y_train_odd = (y_train % 2 == 1)\n",
        "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train, y_multilabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b15389-6967-4636-b886-63afb68cbeb6",
      "metadata": {
        "id": "90b15389-6967-4636-b886-63afb68cbeb6"
      },
      "outputs": [],
      "source": [
        "knn_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad30779-0798-401e-81a8-0b02945bb473",
      "metadata": {
        "id": "3ad30779-0798-401e-81a8-0b02945bb473"
      },
      "outputs": [],
      "source": [
        "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
        "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f94cc2-4dc9-4a37-a95b-aba3d7265f24",
      "metadata": {
        "id": "d0f94cc2-4dc9-4a37-a95b-aba3d7265f24"
      },
      "source": [
        "## Multioutput Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898e19fb-3831-4faa-8c83-b56e9ec99413",
      "metadata": {
        "id": "898e19fb-3831-4faa-8c83-b56e9ec99413"
      },
      "outputs": [],
      "source": [
        "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
        "X_train_mod = X_train + noise\n",
        "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
        "X_test_mod = X_test + noise\n",
        "y_train_mod = X_train\n",
        "y_test_mod = X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b266c00-b72a-4e13-b6ca-1d0a80d48671",
      "metadata": {
        "id": "3b266c00-b72a-4e13-b6ca-1d0a80d48671"
      },
      "outputs": [],
      "source": [
        "some_index = 0\n",
        "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
        "plt.subplot(122); plot_digit(y_test_mod[some_index])\n",
        "save_fig(\"noisy_digit_example_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3961e6-0c97-481b-bb1f-58bf74ca2c0b",
      "metadata": {
        "id": "9c3961e6-0c97-481b-bb1f-58bf74ca2c0b"
      },
      "outputs": [],
      "source": [
        "knn_clf.fit(X_train_mod, y_train_mod)\n",
        "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
        "plot_digit(clean_digit)\n",
        "save_fig(\"cleaned_digit_example_plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec763c89-6511-4eb3-9e26-ad501fab92d4",
      "metadata": {
        "id": "ec763c89-6511-4eb3-9e26-ad501fab92d4"
      },
      "source": [
        "# Chapter 3, Exercise 1\n",
        " Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\n",
        "on the test set. Hint: the KNeighborsClassifier works quite well for this task;\n",
        "you just need to find good hyperparameter values (try a grid search on the\n",
        "weights and n_neighbors hyperparameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4dffbac",
      "metadata": {
        "id": "e4dffbac"
      },
      "outputs": [],
      "source": [
        "#GridSearchCV will use cross-validation to evaluate all possible combinations of hyperparameters so you don't have to tinker\n",
        "# Import the GridSearchCV class for hyperparameter tuning using cross-validation.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a parameter grid to search over. For the KNeighborsClassifier, we're exploring\n",
        "# two hyperparameters: 'weights' (with options 'uniform' and 'distance') and 'n_neighbors'\n",
        "# (with options 3, 4, and 5). This creates a grid of parameter combinations to test.\n",
        "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
        "\n",
        "# Initialize Classifier. This is the model we're tuning.\n",
        "knn_clf = KNeighborsClassifier()\n",
        "# Set up GridSearchCV with the classifier (knn_clf), the parameter grid (param_grid),\n",
        "# and a 5-fold cross-validation (cv=5). Verbose=3 increases the messages printed\n",
        "# to the console so you can track the progress of the search.\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
        "# Fit the GridSearchCV to the training data. This will test all combinations\n",
        "# of parameters in the grid, using 5-fold cross-validation for each combination.\n",
        "# It selects the best combination based on cross-validated performance.\n",
        "grid_search.fit(X_train, y_train)\n",
        "# After fitting, grid_search holds the best model and parameters\n",
        "grid_search_clf.best_score_\n",
        "#best parameters\n",
        "grid_search_clf.best_params_\n",
        "#best model\n",
        "grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612ab254-f6e7-4828-9108-b0093a288347",
      "metadata": {
        "id": "612ab254-f6e7-4828-9108-b0093a288347"
      },
      "outputs": [],
      "source": [
        "# Import the function to compute accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Use the best model found by GridSearchCV to make predictions on the test set.\n",
        "# This uses the model with the optimal hyperparameters found during the grid search.\n",
        "y_pred = grid_search.predict(X_test)\n",
        "# Calculate the accuracy of the model on the test set by comparing the predicted labels (y_pred)\n",
        "# to the true labels (y_test). The accuracy score is the fraction of correct predictions over\n",
        "# the total number of predictions, expressed as a float between 0 and 1, where 1 means perfect accuracy.\n",
        "accuracy_score(y_test, y_pred)\n",
        "# The variable 'accuracy' now holds the accuracy score of the best model found by GridSearchCV\n",
        "# when evaluated on unseen test data. This gives an estimate of the model's generalization ability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6200408-12b3-492f-994e-fdfc911a8b82",
      "metadata": {
        "id": "e6200408-12b3-492f-994e-fdfc911a8b82"
      },
      "source": [
        "# Chapter 3, Exercise 2\n",
        " Write a function that can shift an MNIST image in any direction (left, right, up,\n",
        "or down) by one pixel.5 Then, for each image in the training set, create four shif‐\n",
        "ted copies (one per direction) and add them to the training set. Finally, train your\n",
        "best model on this expanded training set and measure its accuracy on the test set.\n",
        "You should observe that your model performs even better now! This technique of\n",
        "artificially growing the training set is called data augmentation or training set\n",
        "expansion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d62f9b7",
      "metadata": {
        "id": "8d62f9b7"
      },
      "outputs": [],
      "source": [
        "# Define a function to shift an image by dx (delta x) and dy (delta y) pixels.\n",
        "# This can be used for data augmentation, to create more training data from the existing images\n",
        "# by slightly shifting them in any direction.\n",
        "def shift_image(image, dx, dy):\n",
        "    # Reshape the flat image array into a 28x28 matrix, as the original MNIST images are 28x28 pixels.\n",
        "    image = image.reshape((28, 28))\n",
        "    # Use the 'shift' function from scipy.ndimage.interpolation to shift the image.\n",
        "    # 'dy' and 'dx' specify the shift amount along the y and x axes, respectively.\n",
        "    # 'cval' specifies the value to fill past edges of input if mode is 'constant'. Here it's set to 0,\n",
        "    # meaning the empty space created by the shift will be filled with 0s (black).\n",
        "    # 'mode' specifies how the input array is extended beyond its boundaries. 'constant' means pad with a constant value.\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    # Reshape the shifted image back to a flat array before returning it.\n",
        "    return shifted_image.reshape([-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bcea82",
      "metadata": {
        "id": "c7bcea82"
      },
      "outputs": [],
      "source": [
        "image = X_train[1000]\n",
        "shifted_image_down = shift_image(image, 0, 5)\n",
        "shifted_image_left = shift_image(image, -5, 0)\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.subplot(131)\n",
        "plt.title(\"Original\", fontsize=14)\n",
        "plt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.subplot(132)\n",
        "plt.title(\"Shifted down\", fontsize=14)\n",
        "plt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.subplot(133)\n",
        "plt.title(\"Shifted left\", fontsize=14)\n",
        "plt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f6e62f",
      "metadata": {
        "id": "c9f6e62f"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to hold the augmented training data and labels.\n",
        "# Copy original training images.\n",
        "X_train_augmented = [image for image in X_train]\n",
        "# Copy original training labels.\n",
        "y_train_augmented = [label for label in y_train]\n",
        "\n",
        "# Loop over a set of directions to shift the images: right (1, 0), left (-1, 0), down (0, 1), and up (0, -1).\n",
        "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
        "    # For each direction, shift each image in the training set and add the shifted image to the augmented dataset.\n",
        "\n",
        "    for image, label in zip(X_train, y_train):\n",
        "        # Shift the image using the previously defined function.\n",
        "        shifted_image = shift_image(image, dx, dy)\n",
        "# Add the shifted image to the augmented dataset.\n",
        "        X_train_augmented.append(shift_image(image, dx, dy))\n",
        "        # The label remains the same, as the image content hasn't changed category.\n",
        "        y_train_augmented.append(label)\n",
        "# Convert the augmented datasets from lists to numpy arrays for easier handling in machine learning models.\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298ad44b",
      "metadata": {
        "id": "298ad44b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate a random permutation of indices based on the length of the augmented training set.\n",
        "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
        "\n",
        "# Reorder the augmented training images and labels according to the random permutation.\n",
        "# This ensures that the data is shuffled, mixing the original and augmented images,\n",
        "# which is beneficial for training models to prevent any order bias.\n",
        "X_train_augmented = X_train_augmented[shuffle_idx]\n",
        "y_train_augmented = y_train_augmented[shuffle_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3705622a",
      "metadata": {
        "id": "3705622a"
      },
      "outputs": [],
      "source": [
        "knn_clf = KNeighborsClassifier(**grid_search.best_params_)\n",
        "knn_clf.fit(X_train_augmented, y_train_augmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07040435",
      "metadata": {
        "id": "07040435"
      },
      "outputs": [],
      "source": [
        "y_pred = knn_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aee5921-b20d-4466-9352-857187a3e6a2",
      "metadata": {
        "id": "5aee5921-b20d-4466-9352-857187a3e6a2"
      },
      "source": [
        "# Chapter 3, Exercise 3\n",
        " Tackle the Titanic dataset. A great place to start is on Kaggle (https://www.kaggle.com/c/titanic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60c6824",
      "metadata": {
        "id": "c60c6824"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
        "DOWNLOAD_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/\"\n",
        "\n",
        "def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "    for filename in (\"train.csv\", \"test.csv\"):\n",
        "        filepath = os.path.join(path, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            print(\"Downloading\", filename)\n",
        "            urllib.request.urlretrieve(url + filename, filepath)\n",
        "\n",
        "fetch_titanic_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f93771",
      "metadata": {
        "id": "62f93771"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
        "    csv_path = os.path.join(titanic_path, filename)\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5de4d22",
      "metadata": {
        "id": "a5de4d22"
      },
      "outputs": [],
      "source": [
        "train_data = load_titanic_data(\"train.csv\")\n",
        "test_data = load_titanic_data(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95eac105",
      "metadata": {
        "id": "95eac105"
      },
      "outputs": [],
      "source": [
        "#look at attributes\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694776ad",
      "metadata": {
        "id": "694776ad"
      },
      "outputs": [],
      "source": [
        "#set PassengerId as index column\n",
        "train_data = train_data.set_index(\"PassengerId\")\n",
        "test_data = test_data.set_index(\"PassengerId\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506cefe1",
      "metadata": {
        "id": "506cefe1"
      },
      "outputs": [],
      "source": [
        "#get more info - any missing data?\n",
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e2556b",
      "metadata": {
        "id": "b1e2556b"
      },
      "outputs": [],
      "source": [
        "#median age of females\n",
        "train_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n",
        "#to replace null Age attributes with median age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf852a3c",
      "metadata": {
        "id": "cf852a3c"
      },
      "outputs": [],
      "source": [
        "#look at numerical attributes\n",
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a49d4e",
      "metadata": {
        "id": "07a49d4e"
      },
      "outputs": [],
      "source": [
        "#check in boolean\n",
        "train_data[\"Survived\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752ef049",
      "metadata": {
        "id": "752ef049"
      },
      "outputs": [],
      "source": [
        "#look at categorical attributes\n",
        "train_data[\"Pclass\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe394f6",
      "metadata": {
        "id": "1fe394f6"
      },
      "outputs": [],
      "source": [
        "train_data[\"Sex\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e251eb37",
      "metadata": {
        "id": "e251eb37"
      },
      "outputs": [],
      "source": [
        "train_data[\"Embarked\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94132535",
      "metadata": {
        "id": "94132535"
      },
      "outputs": [],
      "source": [
        "#build our preprocessing pipelines, starting with the pipeline for numerical attributes:\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65444e29",
      "metadata": {
        "id": "65444e29"
      },
      "outputs": [],
      "source": [
        "#build the pipeline for the categorical attributes:\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0769506",
      "metadata": {
        "id": "c0769506"
      },
      "outputs": [],
      "source": [
        "#join the two\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
        "\n",
        "preprocess_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", cat_pipeline, cat_attribs),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4af0e6",
      "metadata": {
        "id": "ba4af0e6"
      },
      "outputs": [],
      "source": [
        "X_train = preprocess_pipeline.fit_transform(\n",
        "    train_data[num_attribs + cat_attribs])\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86b504b",
      "metadata": {
        "id": "a86b504b"
      },
      "outputs": [],
      "source": [
        "#get labels\n",
        "y_train = train_data[\"Survived\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9edb26c",
      "metadata": {
        "id": "f9edb26c"
      },
      "outputs": [],
      "source": [
        "#train classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e66a287",
      "metadata": {
        "id": "8e66a287"
      },
      "outputs": [],
      "source": [
        "#make predictions\n",
        "X_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])\n",
        "y_pred = forest_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d78596",
      "metadata": {
        "id": "e6d78596"
      },
      "outputs": [],
      "source": [
        "#use cross validation to determine how good our model is\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
        "forest_scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ba51ef",
      "metadata": {
        "id": "b8ba51ef"
      },
      "outputs": [],
      "source": [
        "#try to get more accurate classifier with SvC\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC(gamma=\"auto\")\n",
        "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
        "svm_scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745adedc",
      "metadata": {
        "id": "745adedc"
      },
      "outputs": [],
      "source": [
        "#plot the ten scores\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot([1]*10, svm_scores, \".\")\n",
        "plt.plot([2]*10, forest_scores, \".\")\n",
        "plt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\n",
        "plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.show()\n",
        "#can further improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855c4a97",
      "metadata": {
        "id": "855c4a97"
      },
      "outputs": [],
      "source": [
        "train_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\n",
        "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4332df65",
      "metadata": {
        "id": "4332df65"
      },
      "outputs": [],
      "source": [
        "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
        "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3157e7-4878-465d-a8f3-a9b6da66dc3b",
      "metadata": {
        "id": "ce3157e7-4878-465d-a8f3-a9b6da66dc3b"
      },
      "source": [
        "# Chapter 3, Exercise 4\n",
        "Build a spam classifier (a more challenging exercise):\n",
        "\n",
        "• Download examples of spam and ham from Apache SpamAssassin’s public\n",
        "dataset (https://homl.info/spamassassin)..\n",
        "\n",
        "• Unzip the datasets and familiarize yourself with the data format.\n",
        "\n",
        "• Split the datasets into a training set and a test set.\n",
        "\n",
        "• Write a data preparation pipeline to convert each email into a feature vector.\n",
        "Your preparation pipeline should transform an email into a (sparse) vector that\n",
        "indicates the presence or absence of each possible word. For example, if all\n",
        "emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email\n",
        "“Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1]\n",
        "(meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is\n",
        "present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\n",
        "each word.\n",
        "\n",
        "You may want to add hyperparameters to your preparation pipeline to control\n",
        "whether or not to strip off email headers, convert each email to lowercase,\n",
        "remove punctuation, replace all URLs with “URL,” replace all numbers with\n",
        "“NUMBER,” or even perform stemming (i.e., trim off word endings; there are\n",
        "Python libraries available to do this).\n",
        "Finally, try out several classifiers and see if you can build a great spam classi‐\n",
        "fier, with both high recall and high precision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d365569-9b5b-4e04-be71-b6af960168ee",
      "metadata": {
        "id": "8d365569-9b5b-4e04-be71-b6af960168ee"
      },
      "outputs": [],
      "source": [
        "#retrive data\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
        "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
        "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
        "\n",
        "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        tar_bz2_file = tarfile.open(path)\n",
        "        tar_bz2_file.extractall(path=spam_path)\n",
        "        tar_bz2_file.close()\n",
        "\n",
        "fetch_spam_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341d93b5-e51f-4f80-9bcd-d351ee8bb73c",
      "metadata": {
        "id": "341d93b5-e51f-4f80-9bcd-d351ee8bb73c"
      },
      "outputs": [],
      "source": [
        "#download emails\n",
        "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
        "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
        "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
        "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8093e575",
      "metadata": {
        "id": "8093e575"
      },
      "outputs": [],
      "source": [
        "len(ham_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a35b8620",
      "metadata": {
        "id": "a35b8620"
      },
      "outputs": [],
      "source": [
        "len(spam_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203d490d",
      "metadata": {
        "id": "203d490d"
      },
      "outputs": [],
      "source": [
        "#use Python's email module to parse these emails (this handles headers, encoding, etc\n",
        "import email\n",
        "import email.policy\n",
        "\n",
        "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
        "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
        "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fdedfb7",
      "metadata": {
        "id": "8fdedfb7"
      },
      "outputs": [],
      "source": [
        "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
        "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fdb99f0",
      "metadata": {
        "id": "1fdb99f0"
      },
      "outputs": [],
      "source": [
        "#look at one example of ham and one example of spam, to get a feel of what the data looks like:\n",
        "print(ham_emails[1].get_content().strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465f03d0",
      "metadata": {
        "id": "465f03d0"
      },
      "outputs": [],
      "source": [
        "#same for spam emails\n",
        "print(spam_emails[6].get_content().strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982f761e",
      "metadata": {
        "id": "982f761e"
      },
      "outputs": [],
      "source": [
        "#Some emails are actually multipart, with images and attachments. Look at the various types of structures we have:\n",
        "def get_email_structure(email):\n",
        "    if isinstance(email, str):\n",
        "        return email\n",
        "    payload = email.get_payload()\n",
        "    if isinstance(payload, list):\n",
        "        return \"multipart({})\".format(\", \".join([\n",
        "            get_email_structure(sub_email)\n",
        "            for sub_email in payload\n",
        "        ]))\n",
        "    else:\n",
        "        return email.get_content_type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "755edeb5",
      "metadata": {
        "id": "755edeb5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def structures_counter(emails):\n",
        "    structures = Counter()\n",
        "    for email in emails:\n",
        "        structure = get_email_structure(email)\n",
        "        structures[structure] += 1\n",
        "    return structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7413fdc7",
      "metadata": {
        "id": "7413fdc7"
      },
      "outputs": [],
      "source": [
        "structures_counter(ham_emails).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2196bd",
      "metadata": {
        "id": "ae2196bd"
      },
      "outputs": [],
      "source": [
        "structures_counter(spam_emails).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2134133d",
      "metadata": {
        "id": "2134133d"
      },
      "outputs": [],
      "source": [
        "#look at the email headers:\n",
        "for header, value in spam_emails[0].items():\n",
        "    print(header,\":\",value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04549400",
      "metadata": {
        "id": "04549400"
      },
      "outputs": [],
      "source": [
        "#focus on the Subject header:\n",
        "spam_emails[0][\"Subject\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f54a6319",
      "metadata": {
        "id": "f54a6319"
      },
      "outputs": [],
      "source": [
        "#split it into a training set and a test set:\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f745d8b7",
      "metadata": {
        "id": "f745d8b7"
      },
      "outputs": [],
      "source": [
        "#start writing the preprocessing functions. First,  a function to convert HTML to plain text.\n",
        "#The following function first drops the <head> section, then converts all <a> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as &gt; or &nbsp;):\n",
        "import re\n",
        "from html import unescape\n",
        "\n",
        "def html_to_plain_text(html):\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
        "    return unescape(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66bb3cc9",
      "metadata": {
        "id": "66bb3cc9"
      },
      "outputs": [],
      "source": [
        "#HTML spam\n",
        "html_spam_emails = [email for email in X_train[y_train==1]\n",
        "                    if get_email_structure(email) == \"text/html\"]\n",
        "sample_html_spam = html_spam_emails[7]\n",
        "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67987c9",
      "metadata": {
        "id": "d67987c9"
      },
      "outputs": [],
      "source": [
        "#resulting plain text\n",
        "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb59a048",
      "metadata": {
        "id": "eb59a048"
      },
      "outputs": [],
      "source": [
        "#function that takes an email as input and returns its content as plain text, no matter its format is:\n",
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # in case of encoding issues\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07e0a45",
      "metadata": {
        "id": "f07e0a45"
      },
      "outputs": [],
      "source": [
        "print(email_to_text(sample_html_spam)[:100], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5d24fc",
      "metadata": {
        "id": "0f5d24fc"
      },
      "outputs": [],
      "source": [
        "#stemming\n",
        "try:\n",
        "    import nltk\n",
        "\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
        "        print(word, \"=>\", stemmer.stem(word))\n",
        "except ImportError:\n",
        "    print(\"Error: stemming requires the NLTK module.\")\n",
        "    stemmer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b25fea5",
      "metadata": {
        "id": "7b25fea5"
      },
      "outputs": [],
      "source": [
        "#way to replace URLs with the word \"URL\"\n",
        "%pip install -q -U urlextract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29bd121",
      "metadata": {
        "id": "a29bd121"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import urlextract # may require an Internet connection to download root domain names\n",
        "\n",
        "    url_extractor = urlextract.URLExtract()\n",
        "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
        "except ImportError:\n",
        "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
        "    url_extractor = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a76d49e",
      "metadata": {
        "id": "6a76d49e"
      },
      "outputs": [],
      "source": [
        "#transformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
        "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            if self.replace_numbers:\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            word_counts = Counter(text.split())\n",
        "            if self.stemming and stemmer is not None:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df4704d",
      "metadata": {
        "id": "1df4704d"
      },
      "outputs": [],
      "source": [
        "#test transformer\n",
        "X_few = X_train[:3]\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "X_few_wordcounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6181cb",
      "metadata": {
        "id": "9f6181cb"
      },
      "outputs": [],
      "source": [
        "#convert word counts to vectors by building  another transformer\n",
        "#whose fit() method will build the vocabulary (an ordered list of the most common words) and whose transform() method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix.\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588786dd",
      "metadata": {
        "id": "588786dd"
      },
      "outputs": [],
      "source": [
        "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
        "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
        "X_few_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de67eb6",
      "metadata": {
        "id": "4de67eb6"
      },
      "outputs": [],
      "source": [
        "X_few_vectors.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b34a2238",
      "metadata": {
        "id": "b34a2238"
      },
      "outputs": [],
      "source": [
        "vocab_transformer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b64863d",
      "metadata": {
        "id": "0b64863d"
      },
      "outputs": [],
      "source": [
        "#train our first spam classifier! Let's transform the whole dataset:\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
        "])\n",
        "\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bb34f9",
      "metadata": {
        "id": "e1bb34f9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
        "score.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a753ec2",
      "metadata": {
        "id": "2a753ec2"
      },
      "outputs": [],
      "source": [
        "#print out the precision/recall we get on the test set:\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "y_pred = log_clf.predict(X_test_transformed)\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd6702c",
      "metadata": {
        "id": "3dd6702c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}